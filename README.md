# Project Mid-term Report Abstract

Autonomous unmanned aerial vehicles (UAVs) are rising in popularity, however, in some cases these autopilot algorithms are only designed to handle nominal conditions. This severely limits the types of conditions these aircraft can operate in. This paper explores the design of a controller to handle stall conditions of UAVs. A deep reinforcement learning (DRL) controller will be trained to this extent using the proximal policy optimization (PPO) method. The agent will be allowed to control four sets of actuators to control roll, pitch, yaw, and thrust with the goal of effectively returning the UAV to steady level flight from a stalled condition. The goal state will be defined by choosing a random roll, pitch, yaw, and airspeed value within the envelope of nominal flight behavior. Likewise, the initial state will be randomly chosen from the stall conditions of the aircraft. These random conditions will allow our controller to recover from a wide variety of conditions and return to nominal flight. The DRL-PPO controller will be evaluated using key metrics against both the linear quadratic regulator (LQR) algorithm and the model predictive control (MPC).
